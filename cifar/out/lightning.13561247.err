/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
You are using a CUDA device ('NVIDIA A100-SXM-64GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Missing logger folder: ./lightning_logs/experiments
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Missing logger folder: ./lightning_logs/experiments
Missing logger folder: ./lightning_logs/experiments
Missing logger folder: ./lightning_logs/experiments
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Missing logger folder: ./lightning_logs/experiments
Missing logger folder: ./lightning_logs/experiments
Missing logger folder: ./lightning_logs/experiments
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Missing logger folder: ./lightning_logs/experiments
2025-03-07 08:11:34.293032: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-07 08:11:34.909654: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-07 08:11:34.910854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-07 08:11:35.008241: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-07 08:11:35.258113: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-07 08:11:37.199844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | model | ResNet | 23.5 M
---------------------------------
23.5 M    Trainable params
0         Non-trainable params
23.5 M    Total params
94.114    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
`Trainer.fit` stopped: `max_epochs=10` reached.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM-64GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

2025-03-07 08:13:42.862986: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-07 08:13:42.898591: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-07 08:13:42.898619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-07 08:13:42.899780: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-07 08:13:42.906264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-07 08:13:44.193406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | model | ResNet | 2.9 M 
---------------------------------
2.9 M     Trainable params
0         Non-trainable params
2.9 M     Total params
11.764    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121584 KiB |  10721 MiB |   1626 GiB |   1626 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   1625 GiB |   1625 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    492 KiB |     17 MiB |      0 GiB |      0 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121728 KiB |  10721 MiB |   1626 GiB |   1626 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   1625 GiB |   1625 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    635 KiB |     17 MiB |      0 GiB |      0 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |   1624 GiB |   1624 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |   1624 GiB |   1624 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      0 GiB |      0 GiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87168 KiB |   1407 MiB | 771465 MiB | 771380 MiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1406 MiB | 770413 MiB | 770331 MiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3460 KiB |      7 MiB |   1052 MiB |   1049 MiB |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     356    |     515    |   29805    |   29449    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   17376    |   17372    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     352    |     511    |   12429    |   12077    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     361    |     515    |   29805    |   29444    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   17376    |   17372    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     357    |     511    |   12429    |   12072    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       7    |      40    |   13934    |   13927    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      33    |    8741    |    8739    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       5    |      21    |    5193    |    5188    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:13:55,982] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:13:55,983] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.006251589977182448, 'preprocessing_with_comm': 0.002283486071974039, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0033428670139983296, <Type.RESHARDING: 'resharding'>: 0.004908666014671326, 'state_converting': 0.005183731089346111, <Type.ALL: 'all'>: 0.014458111952990294})
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |   3225 GiB |   3225 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   3224 GiB |   3224 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      1 GiB |      1 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |   3225 GiB |   3225 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   3224 GiB |   3224 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      1 GiB |      1 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |   3222 GiB |   3222 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |   3221 GiB |   3221 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      1 GiB |      1 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   1487 GiB |   1487 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   1485 GiB |   1485 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      2 GiB |      2 GiB |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |   58906    |   58548    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   34449    |   34445    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |   24457    |   24103    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |   58906    |   58543    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   34449    |   34445    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |   24457    |   24098    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      47    |   27599    |   27590    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   17331    |   17329    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       7    |      45    |   10268    |   10261    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:05,063] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:14:05,065] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.0059103359235450625, 'preprocessing_with_comm': 0.0018356989603489637, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.003220055950805545, <Type.RESHARDING: 'resharding'>: 0.004765252000652254, 'state_converting': 0.005022768978960812, <Type.ALL: 'all'>: 0.013487780001014471})
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |   4825 GiB |   4825 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   4822 GiB |   4822 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      2 GiB |      2 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |   4825 GiB |   4825 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   4822 GiB |   4822 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      2 GiB |      2 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |   4820 GiB |   4820 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |   4818 GiB |   4818 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      2 GiB |      2 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   2222 GiB |   2222 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   2219 GiB |   2219 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      3 GiB |      3 GiB |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |   88007    |   87649    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   51522    |   51518    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |   36485    |   36131    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |   88007    |   87644    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   51522    |   51518    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |   36485    |   36126    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      48    |   41371    |   41362    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   25921    |   25919    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       7    |      46    |   15450    |   15443    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:14,176] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:14:14,178] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005892993998713791, 'preprocessing_with_comm': 0.0017069049645215273, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.003050327068194747, <Type.RESHARDING: 'resharding'>: 0.004754540976136923, 'state_converting': 0.005010838038288057, <Type.ALL: 'all'>: 0.013321832986548543})
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |   6424 GiB |   6424 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   6421 GiB |   6421 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      3 GiB |      3 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |   6424 GiB |   6424 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   6421 GiB |   6421 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      3 GiB |      3 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |   6418 GiB |   6418 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |   6415 GiB |   6415 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      3 GiB |      3 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   2957 GiB |   2957 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   2953 GiB |   2953 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      4 GiB |      4 GiB |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  117108    |  116750    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   68595    |   68591    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |   48513    |   48159    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  117108    |  116745    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   68595    |   68591    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |   48513    |   48154    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      48    |   55130    |   55121    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   34511    |   34509    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       7    |      46    |   20619    |   20612    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:23,295] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:14:23,297] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.00609376304782927, 'preprocessing_with_comm': 0.0018202170031145215, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.003025843994691968, <Type.RESHARDING: 'resharding'>: 0.004776494926773012, 'state_converting': 0.005046634003520012, <Type.ALL: 'all'>: 0.013705478981137276})
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |   8024 GiB |   8023 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   8019 GiB |   8019 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      4 GiB |      4 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |   8024 GiB |   8023 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   8019 GiB |   8019 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      4 GiB |      4 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |   8016 GiB |   8016 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |   8012 GiB |   8012 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      4 GiB |      4 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   3691 GiB |   3691 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   3686 GiB |   3686 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      5 GiB |      5 GiB |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  146209    |  145851    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   85668    |   85664    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |   60541    |   60187    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  146209    |  145846    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   85668    |   85664    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |   60541    |   60182    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |      10    |      48    |   68924    |   68914    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   43101    |   43099    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       8    |      46    |   25823    |   25815    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:32,513] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:14:32,514] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005960982060059905, 'preprocessing_with_comm': 0.0018188809044659138, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0032213079975917935, <Type.RESHARDING: 'resharding'>: 0.004739279975183308, 'state_converting': 0.004994226968847215, <Type.ALL: 'all'>: 0.013494436047039926})
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |   9623 GiB |   9623 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   9618 GiB |   9618 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      5 GiB |      5 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |   9623 GiB |   9623 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |   9618 GiB |   9618 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      5 GiB |      5 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |   9614 GiB |   9614 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |   9609 GiB |   9609 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      5 GiB |      5 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   4426 GiB |   4426 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   4420 GiB |   4420 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      6 GiB |      6 GiB |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  175310    |  174952    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  102741    |  102737    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |   72569    |   72215    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  175310    |  174947    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  102741    |  102737    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |   72569    |   72210    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |      10    |      49    |   82640    |   82630    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   51691    |   51689    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       8    |      47    |   30949    |   30941    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:41,698] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:14:41,700] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005943643976934254, 'preprocessing_with_comm': 0.0018121380126103759, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0031619539950042963, <Type.RESHARDING: 'resharding'>: 0.004710789071395993, 'state_converting': 0.004982898011803627, <Type.ALL: 'all'>: 0.013467774959281087})
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |  11222 GiB |  11222 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |  11216 GiB |  11216 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      5 GiB |      5 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |  11222 GiB |  11222 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |  11216 GiB |  11216 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      5 GiB |      5 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |  11212 GiB |  11212 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |  11206 GiB |  11206 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      5 GiB |      5 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   5160 GiB |   5160 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   5153 GiB |   5153 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      7 GiB |      7 GiB |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  204411    |  204053    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  119814    |  119810    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |   84597    |   84243    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  204411    |  204048    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  119814    |  119810    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |   84597    |   84238    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      49    |   96402    |   96393    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   60281    |   60279    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       7    |      47    |   36121    |   36114    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:14:50,867] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:14:50,869] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.00609374197665602, 'preprocessing_with_comm': 0.001836427953094244, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0031539580086246133, <Type.RESHARDING: 'resharding'>: 0.004651576979085803, 'state_converting': 0.004913114942610264, <Type.ALL: 'all'>: 0.013580144965089858})
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |  12822 GiB |  12822 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |  12815 GiB |  12815 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      6 GiB |      6 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |  12822 GiB |  12822 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |  12815 GiB |  12815 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      6 GiB |      6 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |  12810 GiB |  12810 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |  12803 GiB |  12803 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      6 GiB |      6 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   5895 GiB |   5895 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   5887 GiB |   5887 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      8 GiB |      8 GiB |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  233512    |  233154    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  136887    |  136883    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |   96625    |   96271    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  233512    |  233149    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  136887    |  136883    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |   96625    |   96266    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      49    |  110101    |  110092    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   68871    |   68869    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       7    |      47    |   41230    |   41223    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:15:00,073] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:15:00,074] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.006007796968333423, 'preprocessing_with_comm': 0.0016897498862817883, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0030880779959261417, <Type.RESHARDING: 'resharding'>: 0.004656237899325788, 'state_converting': 0.004921192885376513, <Type.ALL: 'all'>: 0.013347084051929414})
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |  14421 GiB |  14421 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |  14413 GiB |  14413 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      7 GiB |      7 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |  14421 GiB |  14421 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |  14413 GiB |  14413 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      7 GiB |      7 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |  14408 GiB |  14408 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |  14400 GiB |  14400 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      7 GiB |      7 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   6630 GiB |   6630 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   6621 GiB |   6621 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      9 GiB |      8 GiB |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  262613    |  262255    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  153960    |  153956    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |  108653    |  108299    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  262613    |  262250    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  153960    |  153956    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |  108653    |  108294    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |      10    |      49    |  123737    |  123727    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   77461    |   77459    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       8    |      47    |   46276    |   46268    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:15:09,413] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:15:09,415] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.00607205496635288, 'preprocessing_with_comm': 0.0017796549946069717, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0029744120547547936, <Type.RESHARDING: 'resharding'>: 0.004704430000856519, 'state_converting': 0.004972602007910609, <Type.ALL: 'all'>: 0.0135651680175215})
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121585 KiB |  10721 MiB |  16020 GiB |  16020 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |  16012 GiB |  16012 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      8 GiB |      8 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121729 KiB |  10721 MiB |  16020 GiB |  16020 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 121092 KiB |  10721 MiB |  16012 GiB |  16012 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      8 GiB |      8 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |  10701 MiB |  16006 GiB |  16006 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |  10701 MiB |  15997 GiB |  15997 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      8 GiB |      8 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |  13390 MiB |  13390 MiB |  13574 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  13368 MiB |  13368 MiB |  13538 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  87167 KiB |   1638 MiB |   7364 GiB |   7364 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |  83707 KiB |   1637 MiB |   7354 GiB |   7354 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |      7 MiB |      9 GiB |      9 GiB |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  291714    |  291356    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  171033    |  171029    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |  120681    |  120327    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  291714    |  291351    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  171033    |  171029    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |  120681    |  120322    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |     105    |     105    |     117    |      12    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      94    |      94    |      99    |       5    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      49    |  137453    |  137444    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      34    |   86051    |   86049    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       7    |      47    |   51402    |   51395    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:15:18,540] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:15:18,541] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005926101002842188, 'preprocessing_with_comm': 0.0018026919569820166, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.003305425983853638, <Type.RESHARDING: 'resharding'>: 0.0050254230154678226, 'state_converting': 0.005282104015350342, <Type.ALL: 'all'>: 0.013734669890254736})
`Trainer.fit` stopped: `max_epochs=10` reached.
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 104672 KiB |  10721 MiB |  16021 GiB |  16020 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104196 KiB |  10721 MiB |  16012 GiB |  16012 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    475 KiB |     17 MiB |      8 GiB |      8 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 104815 KiB |  10721 MiB |  16021 GiB |  16020 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104196 KiB |  10721 MiB |  16012 GiB |  16012 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    619 KiB |     17 MiB |      8 GiB |      8 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 103954 KiB |  10701 MiB |  16006 GiB |  16006 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 103396 KiB |  10701 MiB |  15997 GiB |  15997 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      8 GiB |      8 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   | 116736 KiB |  13390 MiB |  13574 MiB |  13460 MiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 112640 KiB |  13368 MiB |  13538 MiB |  13428 MiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   4096 KiB |     22 MiB |     36 MiB |     32 MiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  11920 KiB |   1638 MiB |   7364 GiB |   7364 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   8443 KiB |   1637 MiB |   7354 GiB |   7354 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3477 KiB |      7 MiB |      9 GiB |      9 GiB |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     321    |     517    |  292070    |  291749    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |     112    |  171034    |  171032    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     319    |     513    |  121036    |  120717    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     326    |     517    |  292070    |  291744    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |     112    |  171034    |  171032    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     324    |     513    |  121036    |  120712    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |       4    |     105    |     117    |     113    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      94    |      99    |      97    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       2    |      11    |      18    |      16    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       6    |      49    |  137523    |  137517    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       1    |      34    |   86052    |   86051    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       5    |      47    |   51471    |   51466    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:15:19,019] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:15:19,021] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005783068016171455, 'preprocessing_with_comm': 0.0014934430364519358, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0028236829675734043, <Type.RESHARDING: 'resharding'>: 0.004100552061572671, 'state_converting': 0.004353275988250971, <Type.ALL: 'all'>: 0.012331252917647362})
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    return fn(self, **kwargs)
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._accelerator_connector = _AcceleratorConnector(
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    self._accelerator_connector = _AcceleratorConnector(
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    self._check_config_and_set_final_flags(
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
Traceback (most recent call last):
    self._check_config_and_set_final_flags(
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    trainer = L.Trainer(
              ^^^^^^^^^^
    raise ValueError(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
srun: error: lrdn0962: tasks 0-3: Exited with exit code 1
srun: error: lrdn0963: tasks 4-7: Exited with exit code 1
srun: Job 13561247 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 13561247
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
You are using a CUDA device ('NVIDIA A100-SXM-64GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

2025-03-07 08:15:59.572782: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-07 08:15:59.608157: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-07 08:15:59.608187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-07 08:15:59.609341: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-07 08:15:59.615731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-07 08:16:00.854713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | model | ResNet | 23.5 M
---------------------------------
23.5 M    Trainable params
0         Non-trainable params
23.5 M    Total params
94.114    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
[rank0]:[2025-03-07 08:16:12,792] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.00593651900999248, 'preprocessing_with_comm': 0.0036682620411738753, 'state_converting': 0.00010657601524144411, <Type.ALL: 'all'>: 0.010430136928334832})
[rank0]:[2025-03-07 08:16:22,040] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005844152066856623, 'preprocessing_with_comm': 0.003141205059364438, 'state_converting': 9.608897380530834e-05, <Type.ALL: 'all'>: 0.009789329022169113})
[rank0]:[2025-03-07 08:16:31,245] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.0058359900722280145, 'preprocessing_with_comm': 0.002973717055283487, 'state_converting': 9.394693188369274e-05, <Type.ALL: 'all'>: 0.009600528050214052})
[rank0]:[2025-03-07 08:16:40,530] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005825944012030959, 'preprocessing_with_comm': 0.0033404799178242683, 'state_converting': 9.962997864931822e-05, <Type.ALL: 'all'>: 0.009968040976673365})
[rank0]:[2025-03-07 08:16:49,770] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005899976007640362, 'preprocessing_with_comm': 0.0034639990190044045, 'state_converting': 9.806605521589518e-05, <Type.ALL: 'all'>: 0.010151390917599201})
[rank0]:[2025-03-07 08:16:59,011] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.00585029402282089, 'preprocessing_with_comm': 0.0032122929114848375, 'state_converting': 0.00010165595449507236, <Type.ALL: 'all'>: 0.00987488403916359})
[rank0]:[2025-03-07 08:17:08,273] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005851545021869242, 'preprocessing_with_comm': 0.002572680008597672, 'state_converting': 0.00010276597458869219, <Type.ALL: 'all'>: 0.009234038065187633})
[rank0]:[2025-03-07 08:17:17,532] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005877323914319277, 'preprocessing_with_comm': 0.0030787050491198897, 'state_converting': 0.00010571908205747604, <Type.ALL: 'all'>: 0.009771345998160541})
[rank0]:[2025-03-07 08:17:26,772] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005845370003953576, 'preprocessing_with_comm': 0.002717965980991721, 'state_converting': 9.715801570564508e-05, <Type.ALL: 'all'>: 0.009366818936541677})
[rank0]:[2025-03-07 08:17:36,002] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.0058356348890811205, 'preprocessing_with_comm': 0.002924883970990777, 'state_converting': 9.798107203096151e-05, <Type.ALL: 'all'>: 0.009559093974530697})
`Trainer.fit` stopped: `max_epochs=10` reached.
[rank0]:[2025-03-07 08:17:36,394] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005903454031795263, 'preprocessing_with_comm': 0.024097107001580298, 'state_converting': 8.95489938557148e-05, <Type.ALL: 'all'>: 0.03079870401415974})
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
srun: Job 13561247 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 13561247
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM-64GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

2025-03-07 08:18:05.168299: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-07 08:18:05.203226: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-07 08:18:05.203253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-07 08:18:05.204413: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-07 08:18:05.210856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-07 08:18:06.554000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | model | ResNet | 23.5 M
---------------------------------
23.5 M    Trainable params
0         Non-trainable params
23.5 M    Total params
94.114    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
`Trainer.fit` stopped: `max_epochs=10` reached.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM-64GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

2025-03-07 08:20:22.321205: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-07 08:20:22.356379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-07 08:20:22.356408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-07 08:20:22.357552: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-07 08:20:22.364028: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-07 08:20:23.629738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | model | ResNet | 2.9 M 
---------------------------------
2.9 M     Trainable params
0         Non-trainable params
2.9 M     Total params
11.764    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121328 KiB |   2783 MiB |   1667 GiB |   1667 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   1664 GiB |   1664 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    492 KiB |     17 MiB |      3 GiB |      3 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121472 KiB |   2783 MiB |   1667 GiB |   1667 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   1664 GiB |   1664 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    635 KiB |     17 MiB |      3 GiB |      3 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |   1661 GiB |   1661 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |   1658 GiB |   1658 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      3 GiB |      3 GiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107904 KiB | 521760 KiB | 752483 MiB | 752378 MiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB | 748528 MiB | 748426 MiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3460 KiB |   7773 KiB |   3955 MiB |   3951 MiB |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     356    |     515    |  112708    |  112352    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   66150    |   66146    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     352    |     511    |   46558    |   46206    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     361    |     515    |  112708    |  112347    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |   66150    |   66146    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     357    |     511    |   46558    |   46201    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       8    |      63    |   53325    |   53317    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      52    |   32829    |   32826    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       5    |      24    |   20496    |   20491    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:20:36,936] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:20:36,937] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.006074067088775337, 'preprocessing_with_comm': 0.0023271299432963133, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.003411813057027757, <Type.RESHARDING: 'resharding'>: 0.004973485949449241, 'state_converting': 0.005323664052411914, <Type.ALL: 'all'>: 0.014464342035353184})
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |   3327 GiB |   3327 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   3321 GiB |   3321 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      6 GiB |      6 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |   3327 GiB |   3327 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   3321 GiB |   3321 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      6 GiB |      6 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |   3316 GiB |   3316 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |   3310 GiB |   3310 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      6 GiB |      6 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   1465 GiB |   1465 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   1457 GiB |   1457 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |      7 GiB |      7 GiB |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  224724    |  224366    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  132009    |  132005    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |   92715    |   92361    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  224724    |  224361    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  132009    |  132005    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |   92715    |   92356    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       8    |      63    |  106496    |  106488    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |   65474    |   65471    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       5    |      39    |   41022    |   41017    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:20:47,731] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:20:47,732] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005922105978243053, 'preprocessing_with_comm': 0.0019380169687792659, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0027190139517188072, <Type.RESHARDING: 'resharding'>: 0.004466718062758446, 'state_converting': 0.00472150300629437, <Type.ALL: 'all'>: 0.013324421015568078})
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |   4987 GiB |   4987 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   4978 GiB |   4978 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |      9 GiB |      9 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |   4987 GiB |   4987 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   4978 GiB |   4978 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |      9 GiB |      9 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |   4971 GiB |   4971 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |   4962 GiB |   4962 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |      9 GiB |      9 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   2195 GiB |   2195 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   2184 GiB |   2184 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |     11 GiB |     11 GiB |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  336740    |  336382    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  197868    |  197864    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |  138872    |  138518    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  336740    |  336377    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  197868    |  197864    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |  138872    |  138513    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      65    |  159024    |  159015    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |   98119    |   98116    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       6    |      39    |   60905    |   60899    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:20:58,522] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:20:58,523] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005919120041653514, 'preprocessing_with_comm': 0.001626707031391561, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.003023104975000024, <Type.RESHARDING: 'resharding'>: 0.004572656005620956, 'state_converting': 0.004833752987906337, <Type.ALL: 'all'>: 0.013133484055288136})
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |   6647 GiB |   6647 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   6635 GiB |   6635 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |     12 GiB |     12 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |   6647 GiB |   6647 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   6635 GiB |   6635 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |     12 GiB |     12 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |   6626 GiB |   6626 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |   6614 GiB |   6614 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |     12 GiB |     12 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   2926 GiB |   2926 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   2910 GiB |   2910 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |     15 GiB |     15 GiB |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  448756    |  448398    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  263727    |  263723    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |  185029    |  184675    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  448756    |  448393    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  263727    |  263723    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |  185029    |  184670    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       7    |      65    |  211761    |  211754    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |  130764    |  130761    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       4    |      40    |   80997    |   80993    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:09,295] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:21:09,296] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.00594092404935509, 'preprocessing_with_comm': 0.0016659230459481478, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.002958748023957014, <Type.RESHARDING: 'resharding'>: 0.004509489051997662, 'state_converting': 0.0047684779856354, <Type.ALL: 'all'>: 0.013114911038428545})
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |   8307 GiB |   8307 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   8292 GiB |   8292 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |     15 GiB |     15 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |   8307 GiB |   8307 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   8292 GiB |   8292 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |     15 GiB |     15 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |   8281 GiB |   8281 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |   8266 GiB |   8265 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |     15 GiB |     15 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   3656 GiB |   3656 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   3637 GiB |   3637 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |     19 GiB |     19 GiB |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  560772    |  560414    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  329586    |  329582    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |  231186    |  230832    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  560772    |  560409    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  329586    |  329582    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |  231186    |  230827    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      65    |  265140    |  265131    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |  163409    |  163406    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       6    |      40    |  101731    |  101725    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:20,060] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:21:20,062] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.006085005006752908, 'preprocessing_with_comm': 0.0017216689884662628, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.003429393982514739, <Type.RESHARDING: 'resharding'>: 0.0049682240933179855, 'state_converting': 0.005227774963714182, <Type.ALL: 'all'>: 0.013788241078145802})
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |   9967 GiB |   9967 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   9949 GiB |   9949 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |     18 GiB |     18 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |   9967 GiB |   9967 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |   9949 GiB |   9949 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |     18 GiB |     18 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |   9936 GiB |   9936 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |   9917 GiB |   9917 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |     18 GiB |     18 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   4387 GiB |   4387 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   4364 GiB |   4364 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |     23 GiB |     23 GiB |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |  672788    |  672430    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  395445    |  395441    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |  277343    |  276989    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |  672788    |  672425    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |  395445    |  395441    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |  277343    |  276984    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |      10    |      65    |  318082    |  318072    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |  196054    |  196051    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       7    |      40    |  122028    |  122021    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:30,835] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:21:30,836] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.006011073011904955, 'preprocessing_with_comm': 0.001653561950661242, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.003139434033073485, <Type.RESHARDING: 'resharding'>: 0.004673038027249277, 'state_converting': 0.004927521920762956, <Type.ALL: 'all'>: 0.013322721933946013})
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |  11627 GiB |  11627 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |  11606 GiB |  11606 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |     21 GiB |     21 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |  11627 GiB |  11627 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |  11606 GiB |  11606 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |     21 GiB |     21 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |  11591 GiB |  11591 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |  11569 GiB |  11569 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |     21 GiB |     21 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   5117 GiB |   5117 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   5090 GiB |   5090 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |     26 GiB |     26 GiB |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |     784 K  |     784 K  |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |     461 K  |     461 K  |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |     323 K  |     323 K  |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |     784 K  |     784 K  |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |     461 K  |     461 K  |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |     323 K  |     323 K  |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       7    |      65    |  371441    |  371434    |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |  228699    |  228696    |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       4    |      40    |  142742    |  142738    |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:41,941] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:21:41,943] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005946162040345371, 'preprocessing_with_comm': 0.001991490018554032, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.002713814959861338, <Type.RESHARDING: 'resharding'>: 0.004281500005163252, 'state_converting': 0.00454386998899281, <Type.ALL: 'all'>: 0.013221796951256692})
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |  13288 GiB |  13288 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |  13263 GiB |  13263 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |     24 GiB |     24 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |  13288 GiB |  13288 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |  13263 GiB |  13263 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |     24 GiB |     24 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |  13246 GiB |  13246 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |  13221 GiB |  13221 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |     24 GiB |     24 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   5848 GiB |   5848 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   5817 GiB |   5817 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |     30 GiB |     30 GiB |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |     896 K  |     896 K  |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |     527 K  |     527 K  |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |     369 K  |     369 K  |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |     896 K  |     896 K  |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |     527 K  |     527 K  |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |     369 K  |     369 K  |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |      10    |      65    |  424705    |  424695    |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |  261344    |  261341    |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       7    |      40    |  163361    |  163354    |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:21:52,754] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:21:52,755] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.006101064034737647, 'preprocessing_with_comm': 0.0016936680767685175, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0034893580013886094, <Type.RESHARDING: 'resharding'>: 0.0052951889811083674, 'state_converting': 0.005559958983212709, <Type.ALL: 'all'>: 0.014101510983891785})
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |  14948 GiB |  14948 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |  14920 GiB |  14920 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |     27 GiB |     27 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |  14948 GiB |  14948 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |  14920 GiB |  14920 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |     27 GiB |     27 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |  14901 GiB |  14900 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |  14873 GiB |  14873 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |     27 GiB |     27 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   6578 GiB |   6578 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   6544 GiB |   6544 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |     34 GiB |     34 GiB |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |    1008 K  |    1008 K  |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |     593 K  |     593 K  |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |     415 K  |     415 K  |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |    1008 K  |    1008 K  |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |     593 K  |     593 K  |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |     415 K  |     415 K  |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       8    |      65    |  478151    |  478143    |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |  293989    |  293986    |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       5    |      40    |  184162    |  184157    |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:22:03,519] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:22:03,521] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.006054059020243585, 'preprocessing_with_comm': 0.001658463035710156, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0034653369802981615, <Type.RESHARDING: 'resharding'>: 0.00503987492993474, 'state_converting': 0.005300132092088461, <Type.ALL: 'all'>: 0.013763615977950394})
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 121329 KiB |   2783 MiB |  16608 GiB |  16608 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |  16577 GiB |  16577 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    493 KiB |     17 MiB |     30 GiB |     30 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 121473 KiB |   2783 MiB |  16608 GiB |  16608 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120836 KiB |   2782 MiB |  16577 GiB |  16577 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    636 KiB |     17 MiB |     30 GiB |     30 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 120594 KiB |   2768 MiB |  16555 GiB |  16555 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 120036 KiB |   2768 MiB |  16525 GiB |  16524 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |     30 GiB |     30 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   |   3568 MiB |   3568 MiB |   3752 MiB | 188416 KiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   3546 MiB |   3546 MiB |   3716 MiB | 174080 KiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     22 MiB |     22 MiB |     36 MiB |  14336 KiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory | 107903 KiB | 521760 KiB |   7309 GiB |   7309 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104443 KiB | 520384 KiB |   7270 GiB |   7270 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3459 KiB |   7773 KiB |     38 GiB |     38 GiB |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     358    |     517    |    1120 K  |    1120 K  |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |     658 K  |     658 K  |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     354    |     513    |     461 K  |     461 K  |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     363    |     517    |    1120 K  |    1120 K  |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       4    |     112    |     658 K  |     658 K  |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     359    |     513    |     461 K  |     461 K  |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |      87    |      87    |      99    |      12    |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |      76    |      76    |      81    |       5    |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |      11    |      11    |      18    |       7    |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       9    |      65    |  531668    |  531659    |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       3    |      55    |  326634    |  326631    |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       6    |      40    |  205034    |  205028    |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:22:14,310] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:22:14,312] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005887902923859656, 'preprocessing_with_comm': 0.0015950750093907118, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.0030577220022678375, <Type.RESHARDING: 'resharding'>: 0.004575873026624322, 'state_converting': 0.004830120946280658, <Type.ALL: 'all'>: 0.013039489043876529})
`Trainer.fit` stopped: `max_epochs=10` reached.
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] CUDA Memory Summary before calling to _allgather_orig_param_states |===========================================================================|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |                  PyTorch CUDA memory summary, device ID 0                 |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Allocated memory      | 104672 KiB |   2783 MiB |  16608 GiB |  16608 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104196 KiB |   2782 MiB |  16577 GiB |  16577 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    475 KiB |     17 MiB |     30 GiB |     30 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Active memory         | 104815 KiB |   2783 MiB |  16608 GiB |  16608 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 104196 KiB |   2782 MiB |  16577 GiB |  16577 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    619 KiB |     17 MiB |     30 GiB |     30 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Requested memory      | 103954 KiB |   2768 MiB |  16556 GiB |  16555 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 103396 KiB |   2768 MiB |  16525 GiB |  16525 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |    557 KiB |     17 MiB |     30 GiB |     30 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved memory   | 116736 KiB |   3568 MiB |   3752 MiB |   3638 MiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool | 112640 KiB |   3546 MiB |   3716 MiB |   3606 MiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   4096 KiB |     22 MiB |     36 MiB |     32 MiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable memory |  11920 KiB | 521760 KiB |   7309 GiB |   7309 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |   8443 KiB | 520384 KiB |   7270 GiB |   7270 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |   3477 KiB |   7773 KiB |     38 GiB |     38 GiB |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Allocations           |     321    |     517    |    1121 K  |    1120 K  |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |     112    |     658 K  |     658 K  |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     319    |     513    |     462 K  |     462 K  |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Active allocs         |     326    |     517    |    1121 K  |    1120 K  |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |     112    |     658 K  |     658 K  |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |     324    |     513    |     462 K  |     462 K  |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | GPU reserved segments |       4    |      87    |      99    |      95    |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       2    |      76    |      81    |      79    |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       2    |      11    |      18    |      16    |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Non-releasable allocs |       6    |      65    |  531740    |  531734    |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from large pool |       1    |      55    |  326635    |  326634    |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |       from small pool |       5    |      40    |  205105    |  205100    |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize allocations  |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |---------------------------------------------------------------------------|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] | Oversize GPU segments |       0    |       0    |       0    |       0    |
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] |===========================================================================|
[rank0]:[2025-03-07 08:22:14,692] torch.distributed.fsdp._optim_utils: [WARNING] 
[rank0]:[2025-03-07 08:22:14,693] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005925568984821439, 'preprocessing_with_comm': 0.0015008749905973673, <Type.ALLGATHER_OBJ: 'all_gather_object'>: 0.00273789104539901, <Type.RESHARDING: 'resharding'>: 0.004016784951090813, 'state_converting': 0.004270332050509751, <Type.ALL: 'all'>: 0.012399722007103264})
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
srun: Job 13561247 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 13561247
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
Traceback (most recent call last):
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 204, in <module>
    main()
  File "/leonardo/home/userexternal/iguzel00/lightning_multi_gpus/cifar/cifar10_lightning.py", line 148, in main
    trainer = L.Trainer(
              ^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._accelerator_connector = _AcceleratorConnector(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 134, in __init__
    self._check_config_and_set_final_flags(
  File "/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 203, in _check_config_and_set_final_flags
    raise ValueError(
ValueError: You selected an invalid strategy name: `strategy='fsdp1'`. It must be either a string or an instance of `pytorch_lightning.strategies.Strategy`. Example choices: auto, ddp, ddp_spawn, deepspeed, ... Find a complete list of options in our documentation at https://lightning.ai
srun: error: lrdn0962: tasks 0-3: Exited with exit code 1
srun: error: lrdn0963: tasks 4-7: Exited with exit code 1
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
You are using a CUDA device ('NVIDIA A100-SXM-64GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

2025-03-07 08:22:58.592561: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-07 08:22:58.628324: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-07 08:22:58.628351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-07 08:22:58.629500: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-07 08:22:58.635961: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-07 08:22:59.956324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | model | ResNet | 23.5 M
---------------------------------
23.5 M    Trainable params
0         Non-trainable params
23.5 M    Total params
94.114    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
[rank0]:[2025-03-07 08:23:13,701] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005965261021628976, 'preprocessing_with_comm': 0.003544180071912706, 'state_converting': 0.00010462908539921045, <Type.ALL: 'all'>: 0.010339259984903038})
[rank0]:[2025-03-07 08:23:24,921] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005812201066873968, 'preprocessing_with_comm': 0.0028899479657411575, 'state_converting': 9.842205327004194e-05, <Type.ALL: 'all'>: 0.009519829996861517})
[rank0]:[2025-03-07 08:23:36,185] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.0058603930519893765, 'preprocessing_with_comm': 0.0031189959263429046, 'state_converting': 9.669398423284292e-05, <Type.ALL: 'all'>: 0.009788164985366166})
[rank0]:[2025-03-07 08:23:47,497] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.0058226389810442924, 'preprocessing_with_comm': 0.0033297420013695955, 'state_converting': 9.649200364947319e-05, <Type.ALL: 'all'>: 0.009958192007616162})
[rank0]:[2025-03-07 08:23:58,884] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.0058220149949193, 'preprocessing_with_comm': 0.0033116330159828067, 'state_converting': 9.800796397030354e-05, <Type.ALL: 'all'>: 0.00993782700970769})
[rank0]:[2025-03-07 08:24:10,182] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005844973027706146, 'preprocessing_with_comm': 0.002897356986068189, 'state_converting': 9.829492773860693e-05, <Type.ALL: 'all'>: 0.00955619104206562})
[rank0]:[2025-03-07 08:24:21,487] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005824983934871852, 'preprocessing_with_comm': 0.003011372988112271, 'state_converting': 9.920599404722452e-05, <Type.ALL: 'all'>: 0.009641528013162315})
[rank0]:[2025-03-07 08:24:32,886] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005850495072081685, 'preprocessing_with_comm': 0.00310992996674031, 'state_converting': 9.946210775524378e-05, <Type.ALL: 'all'>: 0.00977759703528136})
[rank0]:[2025-03-07 08:24:44,451] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005825856933370233, 'preprocessing_with_comm': 0.002842457965016365, 'state_converting': 9.99419717118144e-05, <Type.ALL: 'all'>: 0.009482070920057595})
[rank0]:[2025-03-07 08:24:55,718] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005847285035997629, 'preprocessing_with_comm': 0.0035101050743833184, 'state_converting': 9.597803000360727e-05, <Type.ALL: 'all'>: 0.010163635015487671})
`Trainer.fit` stopped: `max_epochs=10` reached.
[rank0]:[2025-03-07 08:24:56,042] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.005918327020481229, 'preprocessing_with_comm': 0.0024741420056670904, 'state_converting': 9.09649534150958e-05, <Type.ALL: 'all'>: 0.009214661899022758})
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/leonardo/prod/opt/libraries/cineca-ai/4.3.0/none/cineca-ai-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
